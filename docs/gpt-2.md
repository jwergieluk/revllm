
## GPT-2 Architecture

I am a fan of the Student's t distribution -- it is almost as easy to handle as
the normal distribution but has an additional flexibility with respect to the
heaviness of the tails. In fact, the normal distribution is a special case and
so is the Cauchy distribution.

The density of a (non-degenerate) multivariate normal distribution with zero
mean is given by

$$
\frac{1}{\sqrt{(2\pi)^n |\Sigma|}} \exp \Big( -\frac{1}{2} x^\top \Sigma^{-1} x \Big),
$$

where $$\Sigma$$ is a (positive-semidefinite) $$n\times n$$ covariance matrix.


Reverse Engineering Tools for Large Language Models


## Introduction

RevLLM is a Python library designed to facilitate the analysis of Transformer
Language Models, particularly focusing on generative, decoder-type
transformers. Our library aims to democratize the access to advanced
explainability methods for data scientists and machine learning engineers who
work with language models. Built on top of Andrej Karpathy's esteemed nanoGPT,
RevLLM stands as a robust and user-friendly tool in the field of natural
language processing.

