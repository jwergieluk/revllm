{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T07:52:01.649761278Z",
     "start_time": "2024-01-18T07:51:56.415234461Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from revllm.gpt import GPT, GPTConfig\n",
    "# from revllm.model_wrapper import TokenizerWrapper\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T07:52:17.625998756Z",
     "start_time": "2024-01-18T07:52:01.605243433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "number of parameters: 123.65M\n"
     ]
    }
   ],
   "source": [
    "model_hf = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model_distil = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "model_nano = GPT.from_pretrained('gpt2')\n",
    "model_nano.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "# tokenizer_nano = TokenizerWrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hf(context: str) -> str:\n",
    "    encoded_input = tokenizer(context, return_tensors=\"pt\").to(device)\n",
    "    input_ids = encoded_input[\"input_ids\"].to(device)\n",
    "    attention_mask = encoded_input[\"attention_mask\"].to(device)\n",
    "\n",
    "    output = model_hf.generate(**encoded_input, max_length=len(input_ids) + 15, do_sample=False)\n",
    "\n",
    "    return tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalize to context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial context: What is the capital of France?\n",
      "\n",
      "What is the capital of France?\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "Predicted tokens, with attributions in order:\n",
      "(note: the first tokens are spaces)\n",
      "===========================================\n",
      "Predicted Token: \n",
      "\n",
      "\n",
      "Ġof: 0.4667750298976898\n",
      "Ġis: 0.09483694285154343\n",
      "What: -0.20078647136688232\n",
      "ĠFrance: -0.3289549946784973\n",
      "Ġthe: -0.3516395092010498\n",
      "?: -0.4432636499404907\n",
      "Ġcapital: -0.5517855882644653\n",
      "\n",
      "===========================================\n",
      "Predicted Token: \n",
      "\n",
      "\n",
      "What: -0.06957273185253143\n",
      "Ġof: -0.23914484679698944\n",
      "Ċ: -0.24015489220619202\n",
      "Ġcapital: -0.3045584261417389\n",
      "Ġthe: -0.33885839581489563\n",
      "?: -0.41570526361465454\n",
      "Ġis: -0.4330602288246155\n",
      "ĠFrance: -0.5588936805725098\n",
      "\n",
      "===========================================\n",
      "Predicted Token: \n",
      "\n",
      "\n",
      "?: 0.002080421196296811\n",
      "Ġof: -0.0015987710794433951\n",
      "What: -0.0022416578140109777\n",
      "Ġcapital: -0.0026813633739948273\n",
      "ĠFrance: -0.0035091438330709934\n",
      "Ġthe: -0.00453113904222846\n",
      "Ġis: -0.0058142454363405704\n",
      "ĊĊ: -0.9999570846557617\n",
      "\n",
      "===========================================\n",
      "Predicted Token: The\n",
      "\n",
      "Ġof: 0.16130423545837402\n",
      "Ġis: 0.14146742224693298\n",
      "?: -0.08470432460308075\n",
      "What: -0.21940062940120697\n",
      "Ġthe: -0.2532506287097931\n",
      "Ġcapital: -0.25543317198753357\n",
      "Ċ: -0.4186558425426483\n",
      "ĠFrance: -0.5233495831489563\n",
      "ĊĊ: -0.5657799243927002\n",
      "\n",
      "===========================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(note: the first tokens are spaces)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_tokens_to_generate):\n\u001b[0;32m---> 28\u001b[0m     predicted_token, predicted_token_id, input_ids, attention_mask, attributions \u001b[38;5;241m=\u001b[39m \u001b[43migs_nano\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     attributions_dict[predicted_token] \u001b[38;5;241m=\u001b[39m attributions\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Append the predicted token ID to the input\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 51\u001b[0m, in \u001b[0;36migs_nano\u001b[0;34m(context, n_steps)\u001b[0m\n\u001b[1;32m     49\u001b[0m class_output_at_step \u001b[38;5;241m=\u001b[39m output_at_step[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, predicted_token_id] \u001b[38;5;66;03m#[50]\u001b[39;00m\n\u001b[1;32m     50\u001b[0m summed_output_for_gradient_computation \u001b[38;5;241m=\u001b[39m class_output_at_step\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;66;03m#[1]\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[43msummed_output_for_gradient_computation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# class_output_at_step.backward(retain_graph=True)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m step_embeddings\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ws/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ws/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "start_time = time.time()\n",
    "\n",
    "context = \"What is the capital of France?\"\n",
    "\n",
    "print(f\"Initial context: {context}\")\n",
    "print(\"\")\n",
    "context_with_prediction = predict_hf(context)\n",
    "\n",
    "print(context_with_prediction)\n",
    "print(\"\")\n",
    "\n",
    "num_tokens_to_generate = 9\n",
    "\n",
    "attributions_dict = {}\n",
    "print(\"Predicted tokens, with attributions in order:\")\n",
    "print(\"(note: the first tokens are spaces)\")\n",
    "\n",
    "for _ in range(num_tokens_to_generate):\n",
    "    predicted_token, predicted_token_id, input_ids, attention_mask, attributions = igs_nano(context)\n",
    "    attributions_dict[predicted_token] = attributions\n",
    "\n",
    "    # Append the predicted token ID to the input\n",
    "    predicted_token_tensor = torch.tensor([[predicted_token_id]], dtype=torch.long)\n",
    "    input_ids = torch.cat((input_ids, predicted_token_tensor), dim=1)\n",
    "    context = tokenizer.decode(input_ids[0])\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time elapsed: {end_time - start_time}\")\n",
    "print(\"\")\n",
    "print(tokenizer.decode(input_ids[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For one word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def igs_nano(context: str, n_steps: int = 50) -> tuple[str, int, torch.Tensor]:\n",
    "#     encoded_input = tokenizer(context, return_tensors=\"pt\") #returns a dict\n",
    "#     input_ids = encoded_input[\"input_ids\"]  # [1, 7]\n",
    "#     baseline_input_ids = torch.zeros_like(input_ids) #[1, 7]\n",
    "\n",
    "#     attention_mask = encoded_input[\"attention_mask\"] # [1, 7]\n",
    "#     all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0].detach().tolist())    # [1, 7]\n",
    "\n",
    "#     input_embeddings = model_nano.transformer.wte(input_ids).to(device) #[1, 7, 768]\n",
    "#     baseline_embeddings = model_nano.transformer.wte(baseline_input_ids).to(device) #[1, 7, 768]\n",
    "\n",
    "#     model_nano.eval()\n",
    "\n",
    "#     output_logits = model_nano(input_ids)[0] #[1, 7, 50257]\n",
    "#     next_token_logits = output_logits[0, 0, :]  # [50257]\n",
    "#     predicted_token_id = torch.argmax(next_token_logits).item() # [1]\n",
    "\n",
    "#     position_ids = torch.arange(0, input_embeddings.size(1)).unsqueeze(0) #[1, 7]\n",
    "#     position_embeddings = model_nano.transformer.wpe(position_ids) #[1, 7, 768]\n",
    "\n",
    "# # choose target_word_index\n",
    "#     for target_word_index in range(input_embeddings.size(1)):\n",
    "    \n",
    "#         target_word_embedding = input_embeddings[0,target_word_index,:].unsqueeze(0) #[1, 768]\n",
    "#         target_word_baseline = baseline_embeddings[0,target_word_index,:].unsqueeze(0) #[1, 768]\n",
    "\n",
    "#         alphas = torch.linspace(0, 1, steps=n_steps).unsqueeze(-1) #[50, 1]\n",
    "\n",
    "#         step_embeddings = target_word_baseline + alphas * (target_word_embedding - target_word_baseline) #[50, 768]\n",
    "#         step_embeddings.requires_grad_(True) #[50, 768]\n",
    "#         step_embeddings.retain_grad()\n",
    "#         step_embeddings.grad = None\n",
    "\n",
    "#         forward_embeddings = step_embeddings + position_embeddings #[50.768]\n",
    "#         forward_embeddings = model_nano.transformer.drop(forward_embeddings) #[50, 768]   \n",
    "\n",
    "#         for block in model_nano.transformer.h:\n",
    "#             forward_embeddings = block(forward_embeddings) #[50, 768]\n",
    "\n",
    "#         forward_embeddings = model_nano.transformer.ln_f(forward_embeddings) #[50, 768]\n",
    "#         output_at_step = model_nano.lm_head(forward_embeddings) #[50, 50257]\n",
    "\n",
    "#         class_output_at_step = output_at_step[:, predicted_token_id] #[50]\n",
    "#         summed_output_for_gradient_computation = class_output_at_step.sum() #[1]\n",
    "#         summed_output_for_gradient_computation.backward(retain_graph=True)\n",
    "#         # class_output_at_step.backward(retain_graph=True)\n",
    "#         # DO THINGS\n",
    "\n",
    "#         assert step_embeddings.grad is not None\n",
    "\n",
    "#         step_embeddings_grad_pre_sum = step_embeddings.grad/n_steps #[50, 768]\n",
    "\n",
    "#         target_word_igs = step_embeddings_grad_pre_sum.sum(dim=0) #[1, 768]\n",
    "#         target_word_igs = target_word_igs * (target_word_embedding - target_word_baseline) #[1, 768]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interrupt Model at Will"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"What is the capital of France?\"\n",
    "\n",
    "encoded_input = tokenizer(context, return_tensors='pt')\n",
    "input_ids = encoded_input['input_ids']\n",
    "attention_mask = encoded_input['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_interruption(context: str, \n",
    "                              layer: Literal['word_embeddings', \n",
    "                                             'position_embeddings',\n",
    "                                             'embedding_layer',\n",
    "                                             'block_0',\n",
    "                                             'block_1',\n",
    "                                             'block_2',\n",
    "                                             'block_3',\n",
    "                                             'block_4',\n",
    "                                             'block_5',\n",
    "                                             'block_6',\n",
    "                                             'block_7',\n",
    "                                             'block_8',\n",
    "                                             'block_9',\n",
    "                                             'block_10',\n",
    "                                             'block_11']\n",
    "                               ) -> tuple[int, torch.Tensor]:\n",
    "\n",
    "    blocks_list = ['block_0',\n",
    "                   'block_1',\n",
    "                   'block_2',\n",
    "                   'block_3',\n",
    "                   'block_4',\n",
    "                   'block_5',\n",
    "                   'block_6',\n",
    "                   'block_7',\n",
    "                   'block_8',\n",
    "                   'block_9',\n",
    "                   'block_10',\n",
    "                   'block_11']\n",
    "\n",
    "    input_ids = tokenizer.encode(context, return_tensors='pt')\n",
    "    word_embeddings = model.transformer.wte(input_ids)\n",
    "\n",
    "    if layer == word_embeddings:\n",
    "        extracted_entity = word_embeddings\n",
    "\n",
    "    position_ids = torch.arange(0, input_ids.size(-1)).unsqueeze(0)\n",
    "    position_embeddings = model.transformer.wpe(position_ids)\n",
    "\n",
    "    model_forward_embeddings = word_embeddings + position_embeddings\n",
    "\n",
    "    if layer == position_embeddings:\n",
    "        extracted_entity = model_forward_embeddings\n",
    "\n",
    "    model_forward_embeddings = model.transformer.drop(model_forward_embeddings)\n",
    "\n",
    "    if layer == embedding_layer:\n",
    "        extracted_entity = model_forward_embeddings\n",
    "\n",
    "    block_counter = 0\n",
    "    for block in model.transformer.h:\n",
    "        model_forward_embeddings = block(model_forward_embeddings)[0]\n",
    "        if layer == blocks_list[block_counter]:\n",
    "            extracted_entity = model_forward_embeddings\n",
    "        block_counter += 1\n",
    "\n",
    "    model_forward_embeddings = model.transformer.ln_f(model_forward_embeddings)\n",
    "\n",
    "    model_forward_embeddings = model.lm_head(model_forward_embeddings)\n",
    "\n",
    "    predicted_next_token_logits = model_forward_embeddings[0, -1, :]\n",
    "    predicted_next_token = torch.argmax(predicted_next_token_logits).item()\n",
    "\n",
    "    return predicted_next_token, extracted_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interrupt_at_layer(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-120.2345, -119.8962, -121.1645,  ..., -131.9775, -131.2008,\n",
       "        -116.6154], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_next_token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8ce19da7ef4636a9d7aa04ca1d3132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c675d4a25b1d4a669dc72a26ff80c4d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a75d594093334e2ab86a807370b5cca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1911936c0fe64c0a9eb487294a8aeb46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea0bfb3b66a4e1794d889648f438cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "model = GPT2Model.from_pretrained('distilgpt2')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-5): 6 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
