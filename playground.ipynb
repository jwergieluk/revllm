{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T07:52:01.649761278Z",
     "start_time": "2024-01-18T07:51:56.415234461Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from revllm.gpt import GPT, GPTConfig\n",
    "# from revllm.model_wrapper import TokenizerWrapper\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_intersection_score(prob_tensor_a : torch.tensor, prob_tensor_b: torch.tensor, k: int) -> float:\n",
    "    \"\"\"\n",
    "    Usin intersection scores, as defined in: https://arxiv.org/pdf/2305.13417.pdf\n",
    "    Given: \n",
    "        - two 1D probability tensors\n",
    "        - k: int\n",
    "    Returns: their top k intersection score\n",
    "    \"\"\"\n",
    "\n",
    "    assert all((len(prob_tensor_a.shape) == 1, len(prob_tensor_b.shape) == 1, prob_tensor_a.shape == prob_tensor_b.shape))\n",
    "\n",
    "    topk_a = torch.topk(prob_tensor_a, k).indices.tolist()\n",
    "    topk_b = torch.topk(prob_tensor_b, k).indices.tolist()\n",
    "    intersection = set(topk_a).intersection(set(topk_b))\n",
    "    \n",
    "    return len(intersection) / k\n",
    "\n",
    "def get_top_k_intersection_scores(probabilities_tensor: torch.Tensor, k: int) -> torch.Tensor:\n",
    "\n",
    "    \"\"\"\n",
    "    Given: \n",
    "        - [layer, word, probabilities] tensor\n",
    "        - k: int\n",
    "    Returns: [layer, word, 1] tensor of intersection scores.\n",
    "    \"\"\"\n",
    "\n",
    "    num_layers = probabilities_tensor.shape[0]\n",
    "    num_words = probabilities_tensor.shape[1]\n",
    "\n",
    "    final_layer_probabilities = probabilities_tensor[-1, :, :]\n",
    "\n",
    "    intersection_scores_tensor = torch.zeros(\n",
    "        (num_layers, num_words)\n",
    "    ).unsqueeze(-1)\n",
    "\n",
    "    for layer in range(num_layers):\n",
    "        for word_index in range(num_words):\n",
    "            layer_word_probabilities = probabilities_tensor[layer, word_index, :]\n",
    "            final_word_probabilities = final_layer_probabilities[word_index, :]    \n",
    "            intersection_scores_tensor[layer, word_index] = top_k_intersection_score(\n",
    "                layer_word_probabilities,\n",
    "                final_word_probabilities,\n",
    "                k\n",
    "            )\n",
    "\n",
    "    return intersection_scores_tensor\n",
    "\n",
    "def get_top_k_intersection_scores_control(k: int, probabilities_tensor: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "    \"\"\"\n",
    "    Given a tensor whose last dimension represents probabilities, returns a tensor of intersection scores.\n",
    "    \"\"\"\n",
    "    topk_tensor = torch.topk(probabilities_tensor, k, dim=-1).indices\n",
    "    topk_tensor_final_output = topk_tensor[-1, :, :]\n",
    "    topk_indices_final_output_dict = {}\n",
    "    intersection_scores_tensor = torch.zeros(\n",
    "        (topk_tensor.shape[0], topk_tensor.shape[1])\n",
    "    ).unsqueeze(-1)\n",
    "\n",
    "    for word in range(topk_tensor_final_output.shape[0]):\n",
    "        topk_indices_final_output_dict[word] = set(topk_tensor_final_output[word].tolist())\n",
    "\n",
    "    for layer in range(topk_tensor.shape[0]):\n",
    "        layer_output_topk_tensor = topk_tensor[layer, :, :]\n",
    "        for word in range(layer_output_topk_tensor.shape[0]):\n",
    "            intersection = set(layer_output_topk_tensor[word].tolist()).intersection(\n",
    "                topk_indices_final_output_dict[word]\n",
    "            )\n",
    "            intersection_scores_tensor[layer, word] = len(intersection) / k\n",
    "\n",
    "    return intersection_scores_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0285, 0.0952, 0.1723, 0.0839, 0.1112, 0.3831, 0.0743, 0.0514],\n",
      "        [0.1357, 0.1781, 0.0119, 0.0201, 0.2026, 0.0221, 0.3048, 0.1246]])\n",
      "tensor([[[4, 2, 7],\n",
      "         [6, 0, 2]],\n",
      "\n",
      "        [[6, 7, 4],\n",
      "         [2, 0, 5]],\n",
      "\n",
      "        [[1, 3, 6],\n",
      "         [7, 5, 2]],\n",
      "\n",
      "        [[5, 2, 4],\n",
      "         [6, 4, 1]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6667],\n",
       "         [0.3333]],\n",
       "\n",
       "        [[0.3333],\n",
       "         [0.0000]],\n",
       "\n",
       "        [[0.0000],\n",
       "         [0.0000]],\n",
       "\n",
       "        [[1.0000],\n",
       "         [1.0000]]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp = F.softmax(torch.randn(4,2,8),dim = -1)\n",
    "print(exp[-1,:,:])\n",
    "\n",
    "print(torch.topk(exp, 3, dim=-1).indices)\n",
    "int_scores = get_top_k_intersection_scores(exp, 3)\n",
    "int_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 4 intersection score: 0.75\n",
      "Top 4 a: [2, 8, 5, 6]\n",
      "Top 4 b: [0, 8, 6, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29966/2161400765.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  a = F.softmax(torch.randn(length))\n",
      "/tmp/ipykernel_29966/2161400765.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  b = F.softmax(torch.randn(length))\n"
     ]
    }
   ],
   "source": [
    "length = 10\n",
    "k = 4\n",
    "a = F.softmax(torch.randn(length))\n",
    "b = F.softmax(torch.randn(length))\n",
    "\n",
    "score = top_k_intersection_score(a, b, k)\n",
    "\n",
    "topk_a = torch.topk(a, k).indices.tolist()\n",
    "topk_b = torch.topk(b, k).indices.tolist()\n",
    "\n",
    "print(f\"Top {k} intersection score: {score}\")\n",
    "print(f\"Top {k} a: {topk_a}\")\n",
    "print(f\"Top {k} b: {topk_b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0624, 0.0388, 0.0655, 0.1238, 0.1454, 0.0911, 0.1353, 0.0905, 0.1608,\n",
       "        0.0864])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 5, 4]\n",
      "[9, 3, 6, 0]\n"
     ]
    }
   ],
   "source": [
    "    for word in range(topk_tensor_final_output.shape[0]):\n",
    "        topk_indices_final_output_dict[word] = set(topk_tensor_final_output[word].tolist())\n",
    "\n",
    "    for layer in range(topk_tensor.shape[0]):\n",
    "        layer_output_topk_tensor = topk_tensor[layer, :, :]\n",
    "        for word in range(layer_output_topk_tensor.shape[0]):\n",
    "            intersection = set(layer_output_topk_tensor[word].tolist()).intersection(\n",
    "                topk_indices_final_output_dict[word]\n",
    "            )\n",
    "            intersection_scores_tensor[layer, word] = len(intersection) / k\n",
    "\n",
    "    return intersection_scores_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T07:52:17.625998756Z",
     "start_time": "2024-01-18T07:52:01.605243433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n"
     ]
    }
   ],
   "source": [
    "model = GPT.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"The quick brown fox jumps over the lazy dog\"\n",
    "input_ids = tokenizer.encode(context, return_tensors='pt')\n",
    "position_ids = torch.arange(input_ids.size(1)).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_logits = model(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "* Extract attention scores for each level\n",
    "* I've done it with gpt2 (it's an argument in the model) - easy change to make here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import torch\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2', output_attentions=True)\n",
    "\n",
    "input_ids = tokenizer.encode(context, return_tensors='pt')\n",
    "\n",
    "# Run the model and get attention scores\n",
    "outputs = model(input_ids)\n",
    "attention = outputs.attentions  # Get attention scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mattention\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the layer and head you want to extract scores from\n",
    "layer = 0  # For example, first layer\n",
    "head = 0   # For example, first head\n",
    "attention_scores = attention[layer][0, head]  # 0 for batch index, as we have a single input\n",
    "\n",
    "# attention_scores now contains the attention scores for the specified layer and head\n",
    "# for the input text. It's a matrix of shape (sequence_length, sequence_length),\n",
    "# where each entry [i, j] represents the attention from token i to token j.\n",
    "\n",
    "# To work with or visualize these scores further, you can convert them to numpy, for example:\n",
    "attention_scores_np = attention_scores.detach().numpy()\n",
    "\n",
    "print(attention_scores_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_hf(context: str) -> str:\n",
    "#     encoded_input = tokenizer(context, return_tensors=\"pt\").to(device)\n",
    "#     input_ids = encoded_input[\"input_ids\"].to(device)\n",
    "#     attention_mask = encoded_input[\"attention_mask\"].to(device)\n",
    "\n",
    "#     output = model_hf.generate(**encoded_input, max_length=len(input_ids) + 15, do_sample=False)\n",
    "\n",
    "#     return tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalize to context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial context: What is the capital of France?\n",
      "\n",
      "What is the capital of France?\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "Predicted tokens, with attributions in order:\n",
      "(note: the first tokens are spaces)\n",
      "===========================================\n",
      "Predicted Token: \n",
      "\n",
      "\n",
      "Ġof: 0.4667750298976898\n",
      "Ġis: 0.09483694285154343\n",
      "What: -0.20078647136688232\n",
      "ĠFrance: -0.3289549946784973\n",
      "Ġthe: -0.3516395092010498\n",
      "?: -0.4432636499404907\n",
      "Ġcapital: -0.5517855882644653\n",
      "\n",
      "===========================================\n",
      "Predicted Token: \n",
      "\n",
      "\n",
      "What: -0.06957273185253143\n",
      "Ġof: -0.23914484679698944\n",
      "Ċ: -0.24015489220619202\n",
      "Ġcapital: -0.3045584261417389\n",
      "Ġthe: -0.33885839581489563\n",
      "?: -0.41570526361465454\n",
      "Ġis: -0.4330602288246155\n",
      "ĠFrance: -0.5588936805725098\n",
      "\n",
      "===========================================\n",
      "Predicted Token: \n",
      "\n",
      "\n",
      "?: 0.002080421196296811\n",
      "Ġof: -0.0015987710794433951\n",
      "What: -0.0022416578140109777\n",
      "Ġcapital: -0.0026813633739948273\n",
      "ĠFrance: -0.0035091438330709934\n",
      "Ġthe: -0.00453113904222846\n",
      "Ġis: -0.0058142454363405704\n",
      "ĊĊ: -0.9999570846557617\n",
      "\n",
      "===========================================\n",
      "Predicted Token: The\n",
      "\n",
      "Ġof: 0.16130423545837402\n",
      "Ġis: 0.14146742224693298\n",
      "?: -0.08470432460308075\n",
      "What: -0.21940062940120697\n",
      "Ġthe: -0.2532506287097931\n",
      "Ġcapital: -0.25543317198753357\n",
      "Ċ: -0.4186558425426483\n",
      "ĠFrance: -0.5233495831489563\n",
      "ĊĊ: -0.5657799243927002\n",
      "\n",
      "===========================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(note: the first tokens are spaces)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_tokens_to_generate):\n\u001b[0;32m---> 28\u001b[0m     predicted_token, predicted_token_id, input_ids, attention_mask, attributions \u001b[38;5;241m=\u001b[39m \u001b[43migs_nano\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     attributions_dict[predicted_token] \u001b[38;5;241m=\u001b[39m attributions\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Append the predicted token ID to the input\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 51\u001b[0m, in \u001b[0;36migs_nano\u001b[0;34m(context, n_steps)\u001b[0m\n\u001b[1;32m     49\u001b[0m class_output_at_step \u001b[38;5;241m=\u001b[39m output_at_step[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, predicted_token_id] \u001b[38;5;66;03m#[50]\u001b[39;00m\n\u001b[1;32m     50\u001b[0m summed_output_for_gradient_computation \u001b[38;5;241m=\u001b[39m class_output_at_step\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;66;03m#[1]\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[43msummed_output_for_gradient_computation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# class_output_at_step.backward(retain_graph=True)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m step_embeddings\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ws/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ws/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import time\n",
    "# import warnings\n",
    "\n",
    "# import torch\n",
    "\n",
    "# import logging\n",
    "\n",
    "# logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# start_time = time.time()\n",
    "\n",
    "# context = \"What is the capital of France?\"\n",
    "\n",
    "# print(f\"Initial context: {context}\")\n",
    "# print(\"\")\n",
    "# context_with_prediction = predict_hf(context)\n",
    "\n",
    "# print(context_with_prediction)\n",
    "# print(\"\")\n",
    "\n",
    "# num_tokens_to_generate = 9\n",
    "\n",
    "# attributions_dict = {}\n",
    "# print(\"Predicted tokens, with attributions in order:\")\n",
    "# print(\"(note: the first tokens are spaces)\")\n",
    "\n",
    "# for _ in range(num_tokens_to_generate):\n",
    "#     predicted_token, predicted_token_id, input_ids, attention_mask, attributions = igs_nano(context)\n",
    "#     attributions_dict[predicted_token] = attributions\n",
    "\n",
    "#     # Append the predicted token ID to the input\n",
    "#     predicted_token_tensor = torch.tensor([[predicted_token_id]], dtype=torch.long)\n",
    "#     input_ids = torch.cat((input_ids, predicted_token_tensor), dim=1)\n",
    "#     context = tokenizer.decode(input_ids[0])\n",
    "\n",
    "# end_time = time.time()\n",
    "# print(f\"Time elapsed: {end_time - start_time}\")\n",
    "# print(\"\")\n",
    "# print(tokenizer.decode(input_ids[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For one word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def igs_nano(context: str, n_steps: int = 50) -> tuple[str, int, torch.Tensor]:\n",
    "#     encoded_input = tokenizer(context, return_tensors=\"pt\") #returns a dict\n",
    "#     input_ids = encoded_input[\"input_ids\"]  # [1, 7]\n",
    "#     baseline_input_ids = torch.zeros_like(input_ids) #[1, 7]\n",
    "\n",
    "#     attention_mask = encoded_input[\"attention_mask\"] # [1, 7]\n",
    "#     all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0].detach().tolist())    # [1, 7]\n",
    "\n",
    "#     input_embeddings = model_nano.transformer.wte(input_ids).to(device) #[1, 7, 768]\n",
    "#     baseline_embeddings = model_nano.transformer.wte(baseline_input_ids).to(device) #[1, 7, 768]\n",
    "\n",
    "#     model_nano.eval()\n",
    "\n",
    "#     output_logits = model_nano(input_ids)[0] #[1, 7, 50257]\n",
    "#     next_token_logits = output_logits[0, 0, :]  # [50257]\n",
    "#     predicted_token_id = torch.argmax(next_token_logits).item() # [1]\n",
    "\n",
    "#     position_ids = torch.arange(0, input_embeddings.size(1)).unsqueeze(0) #[1, 7]\n",
    "#     position_embeddings = model_nano.transformer.wpe(position_ids) #[1, 7, 768]\n",
    "\n",
    "# # choose target_word_index\n",
    "#     for target_word_index in range(input_embeddings.size(1)):\n",
    "    \n",
    "#         target_word_embedding = input_embeddings[0,target_word_index,:].unsqueeze(0) #[1, 768]\n",
    "#         target_word_baseline = baseline_embeddings[0,target_word_index,:].unsqueeze(0) #[1, 768]\n",
    "\n",
    "#         alphas = torch.linspace(0, 1, steps=n_steps).unsqueeze(-1) #[50, 1]\n",
    "\n",
    "#         step_embeddings = target_word_baseline + alphas * (target_word_embedding - target_word_baseline) #[50, 768]\n",
    "#         step_embeddings.requires_grad_(True) #[50, 768]\n",
    "#         step_embeddings.retain_grad()\n",
    "#         step_embeddings.grad = None\n",
    "\n",
    "#         forward_embeddings = step_embeddings + position_embeddings #[50.768]\n",
    "#         forward_embeddings = model_nano.transformer.drop(forward_embeddings) #[50, 768]   \n",
    "\n",
    "#         for block in model_nano.transformer.h:\n",
    "#             forward_embeddings = block(forward_embeddings) #[50, 768]\n",
    "\n",
    "#         forward_embeddings = model_nano.transformer.ln_f(forward_embeddings) #[50, 768]\n",
    "#         output_at_step = model_nano.lm_head(forward_embeddings) #[50, 50257]\n",
    "\n",
    "#         class_output_at_step = output_at_step[:, predicted_token_id] #[50]\n",
    "#         summed_output_for_gradient_computation = class_output_at_step.sum() #[1]\n",
    "#         summed_output_for_gradient_computation.backward(retain_graph=True)\n",
    "#         # class_output_at_step.backward(retain_graph=True)\n",
    "#         # DO THINGS\n",
    "\n",
    "#         assert step_embeddings.grad is not None\n",
    "\n",
    "#         step_embeddings_grad_pre_sum = step_embeddings.grad/n_steps #[50, 768]\n",
    "\n",
    "#         target_word_igs = step_embeddings_grad_pre_sum.sum(dim=0) #[1, 768]\n",
    "#         target_word_igs = target_word_igs * (target_word_embedding - target_word_baseline) #[1, 768]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interrupt Model at Will"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context = \"What is the capital of France?\"\n",
    "\n",
    "# encoded_input = tokenizer(context, return_tensors='pt')\n",
    "# input_ids = encoded_input['input_ids']\n",
    "# attention_mask = encoded_input['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_with_interruption(context: str, \n",
    "#                               layer: Literal['word_embeddings', \n",
    "#                                              'position_embeddings',\n",
    "#                                              'embedding_layer',\n",
    "#                                              'block_0',\n",
    "#                                              'block_1',\n",
    "#                                              'block_2',\n",
    "#                                              'block_3',\n",
    "#                                              'block_4',\n",
    "#                                              'block_5',\n",
    "#                                              'block_6',\n",
    "#                                              'block_7',\n",
    "#                                              'block_8',\n",
    "#                                              'block_9',\n",
    "#                                              'block_10',\n",
    "#                                              'block_11']\n",
    "#                                ) -> tuple[int, torch.Tensor]:\n",
    "\n",
    "#     blocks_list = ['block_0',\n",
    "#                    'block_1',\n",
    "#                    'block_2',\n",
    "#                    'block_3',\n",
    "#                    'block_4',\n",
    "#                    'block_5',\n",
    "#                    'block_6',\n",
    "#                    'block_7',\n",
    "#                    'block_8',\n",
    "#                    'block_9',\n",
    "#                    'block_10',\n",
    "#                    'block_11']\n",
    "\n",
    "#     input_ids = tokenizer.encode(context, return_tensors='pt')\n",
    "#     word_embeddings = model.transformer.wte(input_ids)\n",
    "\n",
    "#     if layer == word_embeddings:\n",
    "#         extracted_entity = word_embeddings\n",
    "\n",
    "#     position_ids = torch.arange(0, input_ids.size(-1)).unsqueeze(0)\n",
    "#     position_embeddings = model.transformer.wpe(position_ids)\n",
    "\n",
    "#     model_forward_embeddings = word_embeddings + position_embeddings\n",
    "\n",
    "#     if layer == position_embeddings:\n",
    "#         extracted_entity = model_forward_embeddings\n",
    "\n",
    "#     model_forward_embeddings = model.transformer.drop(model_forward_embeddings)\n",
    "\n",
    "#     if layer == embedding_layer:\n",
    "#         extracted_entity = model_forward_embeddings\n",
    "\n",
    "#     block_counter = 0\n",
    "#     for block in model.transformer.h:\n",
    "#         model_forward_embeddings = block(model_forward_embeddings)[0]\n",
    "#         if layer == blocks_list[block_counter]:\n",
    "#             extracted_entity = model_forward_embeddings\n",
    "#         block_counter += 1\n",
    "\n",
    "#     model_forward_embeddings = model.transformer.ln_f(model_forward_embeddings)\n",
    "\n",
    "#     model_forward_embeddings = model.lm_head(model_forward_embeddings)\n",
    "\n",
    "#     predicted_next_token_logits = model_forward_embeddings[0, -1, :]\n",
    "#     predicted_next_token = torch.argmax(predicted_next_token_logits).item()\n",
    "\n",
    "#     return predicted_next_token, extracted_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# interrupt_at_layer(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8ce19da7ef4636a9d7aa04ca1d3132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c675d4a25b1d4a669dc72a26ff80c4d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a75d594093334e2ab86a807370b5cca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1911936c0fe64c0a9eb487294a8aeb46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea0bfb3b66a4e1794d889648f438cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from transformers import GPT2Tokenizer, GPT2Model\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "# model = GPT2Model.from_pretrained('distilgpt2')\n",
    "# text = \"Replace me by any text you'd like.\"\n",
    "# encoded_input = tokenizer(text, return_tensors='pt')\n",
    "# output = model(**encoded_input)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
