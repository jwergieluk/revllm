## In particular focus - articles

* Bahhacharya, A., (2022). Explainable Machine Learning Models Trained on Text Data: Explaining Text Classifiers Made Easy with Hugging Face Zero Shot Learning with SHAP 
    **Notes**: An application of an existing XAI method SHAP to transofmers.  In initial experiments, I would like to include well-formed examples such as this one.  Applying, understanding, and adapting their application of SHAP is my intended next step.

* Olah, C., Satyanarayan, A., Johnson, I., Carter, S., Schubert, L., Ye, K.Q., & Mordvintsev, A. (2018). The Building Blocks of Interpretability.
    **Notes**: Out of date, but good overview of interpretability

## In Particular Focus - papers

* Ali, A., Schnake, T., Eberle, O., Montavon, G., Müller, K., & Wolf, L. (2022). XAI for Transformers: Better Explanations through Conservative Propagation. ArXiv, abs/2202.07304.
    **Notes**: In focus for two reasons: 1) early sections give a thorough overview of applications of traditional XAI methods to transformers, and 2) they have their own implementation with associated codebase.  Currently have not been able to implement myself, but plan to do so.

* Danilevsky, M., Qian, K., Aharonov, R., Katsis, Y., Kawas, B., & Sen, P. (2020). A Survey of the State of Explainable AI for Natural Language Processing. AACL.
    **Notes**: Survey directly relevant to the topic at hand.  

* Chuang, Y., Xie, Y., Luo, H., Kim, Y., Glass, J.R., & He, P. (2023). DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models. ArXiv, abs/2309.03883.
    **Notes**: Another code implementation, which I intend to look at.

* Scherrer, N., Shi, C., Feder, A., & Blei, D.M. (2023). Evaluating the Moral Beliefs Encoded in LLMs. ArXiv, abs/2307.14324.
    **Notes**: Interesting experiments.

## Potentially relevant, currently not in particular focus

* Amgoud, L., & Ben-Naim, J. (2022). Axiomatic Foundations of Explainability. International Joint Conference on Artificial Intelligence.

* Oswald, J.V., Niklasson, E., Randazzo, E., Sacramento, J., Mordvintsev, A., Zhmoginov, A., & Vladymyrov, M. (2022). Transformers learn in-context by gradient descent. International Conference on Machine Learning.

* Eberle, O., Brandl, S., Pilot, J., & Søgaard, A. (2022). Do Transformer Models Show Similar Attention Patterns to Task-Specific Human Gaze? ArXiv, abs/2205.10226.

* Varma, V., Shah, R., Kenton, Z., Kram'ar, J., & Kumar, R. (2023). Explaining grokking through circuit efficiency. ArXiv, abs/2309.02390.

* Scheurer, J., Campos, J.A., Korbak, T., Chan, J.S., Chen, A., Cho, K., & Perez, E. (2023). Training Language Models with Language Feedback at Scale. ArXiv, abs/2303.16755.

* Roy, A., & Neubauer, M.S. (2022). Interpretability of an Interaction Network for identifying $H \rightarrow b\bar{b}$ jets. Proceedings of 41st International Conference on High Energy physics — PoS(ICHEP2022).

* Dalvi, F., Khan, A.R., Alam, F., Durrani, N., Xu, J., & Sajjad, H. (2022). Discovering Latent Concepts Learned in BERT. ArXiv, abs/2205.07237.

* van Dis, E.A., Bollen, J., Zuidema, W., van Rooij, R., & Bockting, C.L. (2023). ChatGPT: five priorities for research. Nature, 614, 224-226.

* Asperti, A., & Tonelli, V. (2022). Comparing the latent space of generative models. Neural Computing and Applications, 35, 3155 - 3172.

* Chen, A. (2023). Improving Code Generation by Training with Natural Language Feedback. ArXiv, abs/2303.16749.

* Gheini, M., Ren, X., & May, J. (2021). Cross-Attention is All You Need: Adapting Pretrained Transformers for Machine Translation. Conference on Empirical Methods in Natural Language Processing.

* Durrani, N., Sajjad, H., Dalvi, F., & Alam, F. (2022). On the Transformation of Latent Space in Fine-Tuned NLP Models. ArXiv, abs/2210.12696.

* Kim, Y., Denton, C., Hoang, L., & Rush, A.M. (2017). Structured Attention Networks. ArXiv, abs/1702.00887.

* LI, S., Chen, J., Shen, Y., Chen, Z., Zhang, X., Li, Z., Wang, H., Qian, J., Peng, B., Mao, Y., Chen, W., & Yan, X. (2022). Explanations from Large Language Models Make Small Reasoners Better. ArXiv, abs/2210.06726.

* Shi, C., Zheng, C., Feder, A., Vafa, K., & Blei, D.M. (2023). An Invariant Learning Characterization of Controlled Text Generation. ArXiv, abs/2306.00198.

* Grosse, R.B., Bae, J., Anil, C., Elhage, N., Tamkin, A., Tajdini, A., Steiner, B., Li, D., Durmus, E., Perez, E., Hubinger, E., Lukovsiut.e, K., Nguyen, K., Joseph, N., McCandlish, S., Kaplan, J., & Bowman, S. (2023). Studying Large Language Model Generalization with Influence Functions. ArXiv, abs/2308.03296.

* Saeed, W., & Omlin, C.W. (2021). Explainable AI (XAI): A Systematic Meta-Survey of Current Challenges and Future Opportunities. Knowl. Based Syst., 263, 110273.

* Datta, T., & Dickerson, J.P. (2023). Who's Thinking? A Push for Human-Centered Evaluation of LLMs using the XAI Playbook. ArXiv, abs/2303.06223.

* Oswald, J.V., Niklasson, E., Schlegel, M., Kobayashi, S., Zucchet, N., Scherrer, N., Miller, N., Sandler, M., Arcas, B.A., Vladymyrov, M., Pascanu, R., & Sacramento, J. (2023). Uncovering mesa-optimization algorithms in Transformers. ArXiv, abs/2309.05858.

* Many Authors (2022). Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. ArXiv, abs/2206.04615.

* Menick, J., Trebacz, M., Mikulik, V., Aslanides, J., Song, F., Chadwick, M., Glaese, M., Young, S., Campbell-Gillingham, L., Irving, G., & McAleese, N. (2022). Teaching language models to support answers with verified quotes. ArXiv, abs/2203.11147.

* Pires, T., Lopes, A.V., Assogba, Y., & Setiawan, H. (2023). One Wide Feedforward is All You Need. ArXiv, abs/2309.01826.

* Khot, A., Neubauer, M.S., & Roy, A. (2022). A detailed study of interpretability of deep neural network based top taggers. Machine Learning: Science and Technology, 4.

* Wen, Y., Wang, Z., & Sun, J. (2023). MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models. ArXiv, abs/2308.09729.
